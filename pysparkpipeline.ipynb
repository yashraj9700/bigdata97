{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03bcd329-1e86-46d0-a80c-ad898591acf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\python3\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python3\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Model Accuracy: 0.7402597402597403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages if you haven't already (only needed once)\n",
    "# !pip install pyspark\n",
    "\n",
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StructType, StructField\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DiabetesPredictionPipeline\").getOrCreate()\n",
    "\n",
    "# Define the schema for the dataset\n",
    "schema = StructType([\n",
    "    StructField('Pregnancies', IntegerType(), True),\n",
    "    StructField('Glucose', IntegerType(), True),\n",
    "    StructField('BloodPressure', IntegerType(), True),\n",
    "    StructField('SkinThickness', IntegerType(), True),\n",
    "    StructField('Insulin', IntegerType(), True),\n",
    "    StructField('BMI', DoubleType(), True),\n",
    "    StructField('DiabetesPedigreeFunction', DoubleType(), True),\n",
    "    StructField('Age', IntegerType(), True),\n",
    "    StructField('Outcome', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load the data using the defined schema\n",
    "data = spark.read.csv('C:/Users/Yashraj/Downloads/diabaties.csv', schema=schema, header=True)\n",
    "\n",
    "# Columns where 0 values are invalid\n",
    "zero_invalid_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "# Replace 0 values with nulls in invalid columns\n",
    "for col in zero_invalid_cols:\n",
    "    data = data.withColumn(col, F.when(F.col(col) == 0, None).otherwise(F.col(col)))\n",
    "\n",
    "# Define the feature columns\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
    "            'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "\n",
    "# Imputer to fill missing values (zeros replaced by nulls)\n",
    "imputer = Imputer(inputCols=features, outputCols=[f\"{c}_imputed\" for c in features])\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[f\"{c}_imputed\" for c in features], outputCol='features')\n",
    "\n",
    "# Logistic Regression classifier\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Outcome', maxIter=100)\n",
    "\n",
    "# Create a pipeline with imputation, assembling features, and logistic regression\n",
    "pipeline = Pipeline(stages=[imputer, assembler, lr])\n",
    "\n",
    "# Split the dataset into training (70%) and test (30%) sets\n",
    "xtrain, xtest = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Fit the pipeline model on the training data\n",
    "model = pipeline.fit(xtrain)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(xtest)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Output the accuracy\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "# Split the dataset into training and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a pipeline with imputation and logistic regression\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Step 1: Imputation\n",
    "    ('classifier', LogisticRegression(max_iter=1000))  # Step 2: Logistic Regression\n",
    "])\n",
    "\n",
    "# Fit the pipeline model on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbbd50-bc93-4f52-a73a-1e0ce6347b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
